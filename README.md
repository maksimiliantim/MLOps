# Проектирование ML-системы — Поиск товаров (ранжирование по текстовому запросу)

**Курс:** Проектирование систем машинного обучения  
**Тема:** Поиск товаров для e-commerce (ранжирование выдачи по запросу пользователя)  
**Студент:** Тимощук Максим Александрович  
**Группа:** М8О-211СВ-24  

---

## 1. Введение и постановка задачи

Требуется спроектировать поисковую систему для платформы электронной коммерции, которая по текстовому запросу пользователя формирует **ранжированный список товаров**. Ранжирование должно учитывать:
- **текстовую релевантность** (насколько товар соответствует запросу),
- **бизнес-сигналы** (наличие, цена, промо, маржинальность),
- **поведенческие сигналы** (клики, добавления в корзину, покупки).

Система работает **в реальном времени** для Web/Mobile клиентов и собирает логи (запросы, показы, клики, заказы + метрики задержек), которые используются для обучения и переобучения модели ранжирования (LTR, Learning-to-Rank) и последующего деплоя.

### Бизнес-цели
1. **Рост конверсии в покупку** из поиска (доля сессий поиска, завершившихся заказом).  
2. **Рост выручки** (общий доход с заказов, пришедших из поиска).  
3. **Рост среднего чека** (средняя сумма заказа из поиска).  
4. **Рост доли добавлений в корзину** из выдачи поиска.  
5. **Снижение доли «пустых» или нерелевантных выдач** → рост удовлетворённости поиском → удержание пользователей.

### Ключевые нефункциональные требования
- **Задержка ответа** (end-to-end) ≤ **190 мс** на запрос (p95).  
- Масштабирование под пиковую нагрузку (**~12k запросов/сек**, см. расчёт ниже).  
- Надёжный релиз моделей: **канареечный релиз 10% → 100%**, быстрый откат (rollback).  
- Наблюдаемость: метрики/логи/алерты + мониторинг качества модели и бизнес-метрик.

---

## Часть 1. Формулировка ML-задачи и выбор модели

### 1.1 ML-задача

Формулируем задачу как **обучение ранжированию (Learning-to-Rank, LTR)**:

- Для запроса `q` получаем множество кандидатов `D={d1…dn}` (через retrieval-слой).
- Строим функцию скоринга `score(q, di, context)` и сортируем кандидатов по score.
- Возвращаем **Top-K** товаров.

**Входные данные (признаки):**
- признаки запроса: нормализованный текст, токены, бренд/категория, язык, длина запроса;
- признаки товара: категория, атрибуты, текст/название, цена, наличие, промо, рейтинг;
- признаки пары (запрос–товар): BM25-score, близость эмбеддингов, совпадение атрибутов/бренда;
- контекст: устройство, регион, время, сессионные агрегаты;
- онлайн-признаки из **Feature Service / Online Feature Store** (цена/остатки/промо/near-real-time признаки).

**Выход:**
- ранжированный список товаров (Top-K) или score для каждого кандидата.

### 1.2 Разметка (labels)

Лейблы строятся из логов:
- **показ** (impression) — базовый негатив, если не было взаимодействия;
- **клик** (click) — позитив для оптимизации кликабельности;
- **добавление в корзину / покупка** — “сильный” позитив.

На практике удобно использовать **градуированную релевантность** (например 0/1/3) или взвешенную цель: клик < корзина < заказ.

### 1.3 Кандидатные модели

**Модель A: LambdaMART (GBDT-LTR: LightGBM/XGBoost)**
- + быстрый инференс на CPU, проще удерживать задержку;
- + сильный базовый выбор для e-commerce поиска;
- − качество “понимания текста” зависит от текстовых признаков (BM25/эмбеддинги).

**Модель B: Нейросетевой LTR (NN-ранжировщик) + оптимизация инференса (ONNX)**
- + лучше моделирует сложные зависимости и взаимодействия признаков;
- − сложнее удерживать задержку без оптимизаций (ONNX, ограничение Top-N кандидатов).

### 1.4 Выбор

Используем **LTR (LambdaMART или NN) + калибровка**:
- retrieval остаётся двухступенчатым (**BM25/ANN**),
- ранжирование — отдельным сервисом (**Ranking Service**),
- калибровка нужна для стабильного score и корректного сочетания релевантности с бизнес-сигналами.

---

## Часть 2. Проектирование архитектуры

### 2.1 Высокоуровневая архитектура (контур поиска + контур ML)

#### Вход запроса (онлайн-контур)
1. **Clients (Web/Mobile)** отправляют запрос.
2. **CDN / Edge Cache (popular queries)** пытается отдать кеш (hit).
3. При cache miss: запрос идёт в **API Gateway**.
4. **Rate Limiter + Auth** ограничивает трафик и проверяет доступ.
5. Далее запрос маршрутизируется в **Search Platform (Kubernetes)**.

#### Search Platform (Kubernetes): что внутри
Внутри платформы находятся сервисы:

- **Query Understanding (normalize, spelling, brand/category parse)**  
  Подготавливает запрос: нормализация, исправление опечаток, извлечение бренда/категории.

- **Retrieval Service (BM25 / ANN) Top-N candidates**  
  Получает кандидатов из **Search Index (OpenSearch)** (BM25 + ANN vectors).

- **Feature Service (online lookups)**  
  Делает lookup онлайн-признаков (цена/остатки/промо + near-real-time признаки) через **Online Feature Store / KV**.

- **Experiment Router (A/B, canary)**  
  Делит трафик между версиями ранжирования/конфигурациями.

- **Ranking Service (LTR model) relevance + business**  
  Считает score и собирает итоговый Top-K (релевантность + бизнес-сигналы).

Дополнительно:
- **Online Feature Store / KV (Redis/Cassandra)** — быстрые онлайн-признаки.
- **Result Cache (Redis) TTL: 1–5 min** — кеш готовых результатов поиска.

#### Индексация каталога (обновление Search Index)
- **Product Catalog / PIM / DB** → **Indexing Pipeline (ETL + indexing)** → **Search Index (OpenSearch)**.  
Это нужно, чтобы поиск видел актуальные товары и их атрибуты.

#### Сбор событий и контур обучения
- **Search Platform** публикует события **показы/клики/покупки + логи задержек** в **Event Bus (Kafka)**.
- Из Kafka данные попадают в **Data Lake (raw logs) (S3 / Iceberg)**.
- Далее запускается **Training Pipeline (LTR training + calibration) + backtests**.
- Модель и метаданные версионируются в **Model Registry (versioned)**.
- Деплой модели обратно в Search Platform: **deploy model (ONNX)**.

#### Наблюдаемость и управление качеством
- **Метрики сервиса** (задержка p95/p99, запросы/сек, ошибки) — Prometheus.  
- **Логи и трассировки** — ELK.  
- **Мониторинг бизнеса и модели**: кликабельность выдачи, конверсия в покупку, выручка, дрейф/скошенность распределений.  
- При проблемах: **Alerts + Rollback** (автооткат на прошлую версию модели).

<img width="1427" height="715" alt="image" src="https://github.com/user-attachments/assets/25484d1c-86bd-43ac-bd27-905db3a889d3" />

---

### 2.2 Data Pipeline (формирование признаков и обучающего датасета)

1. **Clients/Web/Mobile** генерируют события: запросы, показы, клики, добавления в корзину, заказы.
2. **Event Collector (logging + enrichment)** обогащает (user/device/geo/session) и пишет в:
3. **Event Bus (Kafka)** (search_events, impressions, clicks, orders).
4. **Product Catalog / PIM DB** даёт справочные данные (items, attributes, price, category) и снапшоты.
5. **Data Lake (Iceberg/Parquet)** хранит: raw logs + catalog snapshots.
6. **Data Quality** проверяет schema/null-rate/outliers/freshness.
7. **Batch Processing (Spark) daily/weekly** читает Data Lake.
8. **Data Transformation + Feature Engineering**:
   - join logs + catalog,
   - текстовые признаки,
   - агрегаты (например популярность и кликабельность по запросу/категории),
   - session context.
9. **Labeling / Training Examples** формирует пары (запрос–товар) + метка релевантности.
10. Результаты сохраняются в:
   - **Feature Store (Offline)** — признаки для обучения,
   - **Feature Store (Online)** — признаки для инференса,
   - **Training Dataset** — готовый датасет для LTR.
     
<img width="1345" height="401" alt="image" src="https://github.com/user-attachments/assets/4fab7541-7897-4446-ae58-756beb172a1d" />

---

### 2.3 Training Pipeline (обучение, оценка, регистрация, деплой)

1. Триггеры:
   - **Schedule (daily/weekly)**,
   - **Git push (code/model changes)**.
2. **Orchestrator (Airflow/Argo)** запускает training DAG.
3. Источник данных: **Training Data Source (Feature Store Offline)**.
4. **Data Validation** (schema/leakage/freshness).
5. **Data Split train/val/test** (обычно time-based).
6. **Hyperparameter Config** (model, loss, weights).
7. **Model Training Job LTR (LambdaMART/NN) + calibration**.
8. **Backtesting / Replay** (офлайн-симуляция на логах).
9. **Experiment Tracking (MLflow)** (метрики, параметры).
10. **Evaluation**:
   - NDCG@K, MAP@K,
   - оценка влияния на кликабельность/конверсию в офлайн-режиме (proxy).
11. **Quality Gates**:
   - минимальный NDCG,
   - ограничения по “прокси задержки” (чтобы модель не была слишком тяжёлой).
12. При успехе:
   - сохраняем **Model Artifacts (ONNX, checkpoints)**,
   - регистрируем в **Model Registry (versioned)**,
   - запускаем **Deploy Pipeline: Docker build → K8s canary → full rollout**,
   - уведомления: **Slack/Email + audit logs**.

<img width="1448" height="421" alt="image" src="https://github.com/user-attachments/assets/468b1ff8-c785-4cf3-97e7-cde10418ab6b" />

---

### 2.4 Inference Pipeline (обработка запросов в проде)

1. **Clients** → **CDN/Edge Cache**.
2. При cache miss: **API Gateway** → **Rate Limiter + Auth** → **Search Platform (Kubernetes)**.
3. Внутри Search Platform:
   - Query Understanding → Retrieval Service → Feature Service → Experiment Router → Ranking Service.
4. Retrieval обращается к:
   - **Search Index (OpenSearch) BM25 + ANN vectors**.
5. Feature Service обращается к:
   - **Online Feature Store / KV**.
6. Результат:
   - пишется в **Result Cache (Redis) TTL 1–5 min**,
   - отдаётся пользователю.
7. Параллельно:
   - события (показы/клики/заказы) → **Kafka**,
   - метрики (задержка/запросы в секунду/ошибки) → **Prometheus**,
   - логи → **ELK**,
   - алерты → **Alertmanager + rollback**.
8. Управление релизом:
   - **Canary / Rollout Controller (10% → 100%)**,
   - версии моделей берутся из **Model Registry (versioned)**.

<img width="1344" height="762" alt="image" src="https://github.com/user-attachments/assets/5fa0039d-b0f6-4a52-85d6-a20e516a8046" />

---

### 2.5 Data Storage (что где хранится и зачем)

- **Product Catalog / PIM DB (SSOT for items)** — источник истины по товарам/атрибутам.
- **Indexing Pipeline** обновляет:
  - **Search Index (OpenSearch) (BM25 + ANN vectors)** — индекс для retrieval.
- Онлайн-контур использует:
  - **Redis Result Cache (TTL 1–5 min)** — кеш выдачи,
  - **Online Feature Store / KV (TTL/near-real-time)** — признаки для инференса,
  - **Results DB (PostgreSQL) (last 30 days)** — история выдач/аудит/аналитика “короткого хвоста”.
- События и обучение:
  - **Event Bus (Kafka)** — поток событий поиска,
  - **Data Lake (Iceberg/Parquet) raw logs + training datasets** — сырьё + датасеты,
  - **Iceberg Snapshots** — историчность/воспроизводимость для retraining,
  - **Offline Feature Store (batch features)** — признаки для обучения,
  - **Model Artifacts Store (ONNX, checkpoints)** — артефакты моделей,
  - **Object Storage (S3) (backups, log archive)** — архив логов/бэкапы.

<img width="1294" height="569" alt="image" src="https://github.com/user-attachments/assets/bd89d5fb-ce0f-4f7c-843c-aa40e35e0086" />

---

## Часть 3. Расчёты и нефункциональные требования

### 3.1 Расчёт среднего и пикового RPS (запросов в секунду)

**Дано:**
- DAU = **3 858 438** пользователей/день.
- Примем среднее число поисковых запросов: **25 запросов/пользователь/день** (оценочное допущение).

**Запросов в сутки:**
- `Q_day = DAU × 25 = 3 858 438 × 25 = 96 460 950 запросов/день`

**Средний RPS:**
- `RPS_avg = Q_day / 86 400 ≈ 96 460 950 / 86 400 ≈ 1 116 запросов/сек`

Чтобы оценить **пиковый RPS**, вводим коэффициент пиковости (типичный дневной профиль трафика e-commerce).
Если принять peak factor ~ **10.8×**, то:
- `RPS_peak ≈ 1 116 × 10.8 ≈ 12 053 запросов/сек`

Итого, пиковая нагрузка **~12k запросов/сек** реалистична.

---

### 3.2 Оценка объёма логов (Data Lake)

Допущения на 1 запрос:
- 1 search_event (~3 KB)
- ~20 impressions (~0.5 KB каждая)
- ~2 clicks (~0.5 KB каждая)
- ~0.1 order (~1 KB)

**Запросов/день:** ~96.5 млн

**Объём/день:**
- search_events: `96.5M × 3 KB ≈ 276 GB/day`
- impressions: `96.5M × 20 × 0.5 KB ≈ 920 GB/day`
- clicks: `96.5M × 2 × 0.5 KB ≈ 92 GB/day`
- orders: `96.5M × 0.1 × 1 KB ≈ 9.2 GB/day`

**Итого:** ~**1.30 TB/day** (без учёта компрессии, служебных полей и ретраев).

**За 30 дней:** `1.30 × 30 ≈ 39 TB`

С запасом на ретраи, дополнительные поля, metadata Iceberg и промежуточные витрины обычно закладывают **~1.5–2×**:
- ориентир: **60–80 TB** на 30 дней хранения/работы пайплайна.

---

### 3.3 Оценка сетевого трафика на пике

Пусть payload запроса ~2 KB, ответ ~3 KB.

**Входящий трафик (peak):**
- `12 051 запрос/сек × 2 KB ≈ 24 MB/s`

**Исходящий трафик (peak):**
- `12 051 запрос/сек × 3 KB ≈ 36 MB/s`

Внутренние обращения (OpenSearch + Online KV) контролируются:
- ограничением Top-N кандидатов,
- кешированием **Result Cache (TTL 1–5 мин)**,
- горизонтальным масштабированием retrieval/ranking.

---

### 3.4 Итоговые нефункциональные требования

**Задержка (Latency)**
- p95 ≤ 190 мс.  
Достигается:
- CDN/Result Cache для популярных запросов,
- быстрый retrieval (OpenSearch BM25/ANN),
- быстрые lookup в Online Feature Store,
- оптимизированный ранжировщик (ONNX, лимит Top-N).

**Масштабируемость**
- Автомасштабирование K8s по нагрузке отдельно для Query Understanding / Retrieval / Ranking.
- Kafka — партиции и репликация.
- OpenSearch — шардинг + реплики.
- Batch Spark — отдельный пул под daily/weekly вычисления.

**Надёжность**
- Канареечный релиз (10% → 100%),
- быстрый откат (rollback) по алертам,
- репликация данных и бэкапы в S3.

**Наблюдаемость**
- Метрики сервиса: задержка p95/p99, запросы/сек, ошибки.
- Логи/трассировки: поиск причин деградации.
- Мониторинг бизнеса и модели:
  - **кликабельность выдачи** (CTR),
  - **конверсия в покупку** (CVR),
  - **выручка из поиска**,
  - **средний чек из поиска**,
  - **дрейф данных/признаков** и “скошенность” распределений (skew).

**Безопасность**
- Rate Limiter + Auth,
- аудит релизов и доступов (audit logs),
- разграничение прав на Data Lake/Registry/Feature Store.

---

## Источники
1. Материалы курса (data pipeline, training, deployment, monitoring).  
2. Сюй А. *System Design. Подготовка к сложному интервью* (принципы проектирования высоконагруженных систем).  
3. Практики Kafka/Kubernetes/OpenSearch для online-search и ML-serving.
